Index: ixgbe.h
===================================================================
--- ixgbe.h	(revision 532)
+++ ixgbe.h	(working copy)
@@ -179,6 +179,7 @@
 /* Tx Descriptors needed, worst case */
 //TODO: this may need to be updated for S/G Segmentation
 #define TXD_USE_COUNT(S) DIV_ROUND_UP((S), IXGBE_MAX_DATA_PER_TXD)
+//#define TXD_SGSEGS_USE_COUNT(S, GSO_SIZE) DIV_ROUND_UP((S), (GSO_SIZE))
 #define DESC_NEEDED (MAX_SKB_FRAGS + 4)
 
 /* wrapper around a pointer to a socket buffer,
@@ -198,8 +199,26 @@
         //has been consumed.
         //TODO: size_t is probably too big if we set a reasonable upper
         //bound on the amount of memory pre-allocated for packet
-        //headers. u32?
+        //headers. i32?
+        bool hr_i_valid;
         ssize_t hr_i;
+
+        // Use for mapping all of the DMA segments of an skb in a batch
+        // because they will all eventually be used.  This is wasteful of
+        // memory because this will only ever be used in the "first" tx_buffer
+        // even though we're allocating space for it for all tx_buffers.
+        struct {
+            DEFINE_DMA_UNMAP_ADDR(fdma);
+            DEFINE_DMA_UNMAP_ADDR(flen);
+        } frag_dma[MAX_SKB_FRAGS];
+        //XXX: BUG: This should be MAX GSO SEGs!
+        //bool frag_hr_i_valid[MAX_SKB_FRAGS];
+        //ssize_t frag_hr_i[MAX_SKB_FRAGS];
+
+        /* Included so more context descriptors can be created. */
+	u32 vlan_macip_lens;
+        u32 type_tucmd;
+	u32 mss_l4len_idx;
 };
 
 struct ixgbe_rx_buffer {
@@ -269,7 +288,8 @@
 //TODO: 512 is the maximum header size the ixgbe will handle. What is a
 //typical header size? We could size the descriptors according to
 //that.
-#define IXGBE_MAX_HDR_BYTES                     (512)
+//#define IXGBE_MAX_HDR_BYTES                     (512)
+#define IXGBE_MAX_HDR_BYTES                     (256) //hdr_len is a u8, so 256 is clearly a driver internal max on header length
 struct ixgbe_pkt_hdr {
     u8  raw[IXGBE_MAX_HDR_BYTES];
 };
@@ -300,6 +320,9 @@
         size_t header_ring_len;
         dma_addr_t header_ring_dma;     /* The dma handle to the header ring. */
 
+        //XXX: Unless hr spots are allocated as lazy as possible, this current
+        // allocation scheme is likely not appropriate!  This needs to be
+        // revisited.
         /* The following three variables are in units of ixgbe_pkt_hdr slots,
          * which should be 512 bytes each.  */
         size_t hr_count;
Index: ixgbe_main.c
===================================================================
--- ixgbe_main.c	(revision 532)
+++ ixgbe_main.c	(working copy)
@@ -935,6 +935,7 @@
 	tx_buffer->next_to_watch = NULL;
 	tx_buffer->skb = NULL;
         tx_buffer->hr_i = -1;
+        tx_buffer->hr_i_valid = false;
 	dma_unmap_len_set(tx_buffer, len, 0);
 	/* tx_buffer must be completely set up in the transmit path */
 }
@@ -1157,6 +1158,7 @@
 		tx_buffer->skb = NULL;
 		dma_unmap_len_set(tx_buffer, len, 0);
                 tx_buffer->hr_i = -1;
+                tx_buffer->hr_i_valid = false;
 
 		/* unmap remaining buffers */
 		while (tx_desc != eop_desc) {
@@ -7061,6 +7063,9 @@
 	first->gso_segs = skb_shinfo(skb)->gso_segs;
 	first->bytecount += (first->gso_segs - 1) * *hdr_len;
 
+        //DEBUG: Is gso_segs already set correctly?
+        //pr_info ("gso_segs: %d\n", first->gso_segs);
+
 	/* mss_l4len_id: use 0 as index for TSO */
 	mss_l4len_idx = l4len << IXGBE_ADVTXD_L4LEN_SHIFT;
 	mss_l4len_idx |= skb_shinfo(skb)->gso_size << IXGBE_ADVTXD_MSS_SHIFT;
@@ -7073,6 +7078,11 @@
 	ixgbe_tx_ctxtdesc(tx_ring, vlan_macip_lens, 0, type_tucmd,
 			  mss_l4len_idx);
 
+        /* update first so that the context descriptor can be recreated. */
+        first->vlan_macip_lens = vlan_macip_lens;
+        first->type_tucmd = type_tucmd;
+        first->mss_l4len_idx = mss_l4len_idx;
+
 	return 1;
 }
 
@@ -7482,6 +7492,7 @@
             tx_desc->read.olinfo_status = 0;
             tx_buffer = &tx_ring->tx_buffer_info[i];
             tx_buffer->hr_i = -1;
+            tx_buffer->hr_i = false;
         }
 
 
@@ -7620,6 +7631,128 @@
         tx_ring->hr_next_to_use = hr_i;
 }
 
+static int ixgbe_tx_prepare_skb_sgsegs(struct ixgbe_ring *tx_ring,
+                                       struct ixgbe_tx_buffer *first)
+{
+        struct sk_buff *skb = first->skb;
+	struct skb_frag_struct *frag;
+	unsigned int size;
+        unsigned int frag_i;
+	dma_addr_t dma;
+
+        /* Perform all DMA mapping as a batch because it will eventually need
+         * to be performed. */
+
+        /* assert that this tx_buffer was cleaned properly */
+        BUG_ON (first->len != 0);
+
+        /* First map the skb head. */
+	size = skb_headlen(skb);
+	dma = dma_map_single(tx_ring->dev, skb->data, size, DMA_TO_DEVICE);
+        if (dma_mapping_error(tx_ring->dev, dma))
+                goto dma_error;
+        dma_unmap_len_set(first, len, size);
+        dma_unmap_addr_set(first, dma, dma);
+
+        /* Then map all of the fragments. */
+        frag_i = 0;
+	for (frag = &skb_shinfo(skb)->frags[0];; frag++) {
+                /* map the fragment. */
+		size = skb_frag_size(frag);
+		dma = skb_frag_dma_map(tx_ring->dev, frag, 0, size,
+				       DMA_TO_DEVICE);
+
+                /* check for errors. */
+                if (dma_mapping_error(tx_ring->dev, dma))
+                        goto dma_error;
+
+                /* assert that this tx_buffer was cleaned properly */
+                BUG_ON (first->frag_dma[frag_i].flen != 0);
+
+		/* record length, and DMA address */
+		dma_unmap_len_set(first, frag_dma[frag_i].flen, size);
+		dma_unmap_addr_set(first, frag_dma[frag_i].fdma, dma);
+
+                /* iterating without a loop variable and then updating it at
+                 * the end is dumb. */
+                frag_i++;
+        }
+
+        return 0; 
+
+dma_error:
+	dev_err(tx_ring->dev, "TX DMA map failed\n");
+
+        //TODO: Clean up succesful mappings
+
+        //TODO: free the skb, which can be done through
+        // ixgbe_unmap_and_free_tx_resource
+
+        BUG ();
+
+        return -1;
+}
+
+static void ixgbe_tx_enqueue_sgsegs(struct ixgbe_ring *tx_ring,
+                                    struct ixgbe_tx_buffer *first,
+                                    const u8 hdr_len,
+                                    const u32 seg_offset)
+{
+}
+
+static void ixgbe_tx_map_sgsegs(struct ixgbe_ring *tx_ring,
+			        struct ixgbe_tx_buffer *first,
+			        const u8 hdr_len)
+{
+        struct sk_buff *skb = first->skb;
+	int err = 0;
+        unsigned short gso_seg = 0;
+        u32 gso_size = skb_shinfo(skb)->gso_size;
+        u32 seg_offset = 0;
+        u32 seg_len = skb->len - hdr_len;
+
+
+        //XXX: Because ixgbe_tso has already been called, our starting
+        // descriptor is actually one earlier.
+        u16 start_ntu = tx_ring->next_to_use;
+        u16 end_ntu;
+
+        //DEBUG
+        pr_err ("first->gso_segs: %d\n", first->gso_segs);
+        pr_err ("skb gso_size: %d\n", skb_shinfo(skb)->gso_size);
+
+        /* dma map all of the necessary fragments at once. */
+        err = ixgbe_tx_prepare_skb_sgsegs (tx_ring, first);
+        if (err) {
+            pr_err ("Dropping segment due to prepare errors (DMA)\n");
+            return;
+        }
+
+        /* TODO: possibly copy all of the headers for all of the segments at
+         * once now as well. */
+
+        //NOTE: I'm not sure that generating TSO context descriptors for a
+        // single MTU sized packet will work correctly, but I'm going to try
+        // for now.
+        
+        for (gso_seg = 0; gso_seg < first->gso_segs; gso_seg++) {
+            ixgbe_tx_enqueue_sgsegs (tx_ring, first, hdr_len, seg_offset);
+            //XXX: buggy, we may write less than gso size
+            seg_offset += gso_size;
+            seg_len -= gso_size;
+        }
+
+        // Sanity check that we sent all data
+        BUG_ON (seg_len != 0);
+
+        //DEBUG: how many descriptors were actually consumed?
+        end_ntu = tx_ring->next_to_use;
+        pr_info (" start_ntu: %d\n", start_ntu);
+        pr_info (" end_ntu: %d\n", end_ntu);
+
+        return;
+}
+
 static void ixgbe_atr(struct ixgbe_ring *ring,
 		      struct ixgbe_tx_buffer *first)
 {
@@ -7789,11 +7922,29 @@
 	int tso;
 	u32 tx_flags = 0;
 	unsigned short f;
-	u16 count = TXD_USE_COUNT(skb_headlen(skb));
+	u16 count = 0;
         size_t hr_count = 1;
 	__be16 protocol = skb->protocol;
 	u8 hdr_len = 0;
 
+        // Assume that all current forwarding is SG segmentation.
+        /* need: 3 descriptors per segment
+         *          (+ 1 desc for context descriptor)
+         *          (+ 1 desc for header descriptor)
+         *          (+ 1 desc for data descriptor)
+	 *       + 2 desc gap to keep tail from touching head,
+	 * otherwise try next time
+         */
+        count = (skb_shinfo(skb)->gso_segs * 3);
+        count += 2;
+        hr_count = (skb_shinfo(skb)->gso_segs); // -1 if the first packet
+                                                // uses the existing header
+
+// Assume that all forwarding is performed by SG segmentation, thus all
+// packets have the requirements of SG segmentation.  This will not always be
+// the case.  Deciding if there are enough descriptors should be decided
+// algorithmically in the future.
+#if 0
 	/*
 	 * need: 1 descriptor per page * PAGE_SIZE/IXGBE_MAX_DATA_PER_TXD,
 	 *       + 1 desc for skb_headlen/IXGBE_MAX_DATA_PER_TXD,
@@ -7802,30 +7953,33 @@
          *       (+ 1 desc gap for the header located in the header ring)
 	 * otherwise try next time
 	 */
+        count = TXD_USE_COUNT(skb_headlen(skb));
 	for (f = 0; f < skb_shinfo(skb)->nr_frags; f++)
 		count += TXD_USE_COUNT(skb_shinfo(skb)->frags[f].size);
-
+        //TODO: Pick this depending on how segments are being generated
+        //count += 3;
+        count += 4; // Include the header ring
         /* 
          * need: 1 hr slot per segment for now.
          * in the future: 1 hr slot per packet in a segment
          */
-        // This should always hold now, but may not in the future. */
-        BUG_ON(hr_count > ixgbe_hr_unused(tx_ring));
+        hr_count = 1;
+#endif
 
-        //TODO: Pick this depending on how segments are being generated
-        //count += 3;
-        count += 4; // Include the header ring
-
 	if (ixgbe_maybe_stop_tx(tx_ring, count)) {
 		tx_ring->tx_stats.tx_busy++;
 		return NETDEV_TX_BUSY;
 	}
 
+        // This should always hold now, but may not in the future. */
+        BUG_ON(hr_count > ixgbe_hr_unused(tx_ring));
+
 	/* record the location of the first descriptor for this packet */
 	first = &tx_ring->tx_buffer_info[tx_ring->next_to_use];
 	first->skb = skb;
 	first->bytecount = skb->len;
 	first->gso_segs = 1;
+        first->hr_i_valid = false;
         first->hr_i = -1;
 
 	/* if we have a HW VLAN tag being added default to the HW one */
@@ -7905,6 +8059,7 @@
 	}
 
 #endif /* IXGBE_FCOE */
+
 	tso = ixgbe_tso(tx_ring, first, &hdr_len);
 	if (tso < 0)
 		goto out_drop;
@@ -7919,9 +8074,13 @@
 xmit_fcoe:
 #endif /* IXGBE_FCOE */
 
-        //TODO: This should be decided algorithmically in the future.
-	//ixgbe_tx_map(tx_ring, first, hdr_len);
-	ixgbe_tx_map_hdr_ring(tx_ring, first, hdr_len);
+        if (tso) {
+            ixgbe_tx_map_sgsegs(tx_ring, first, hdr_len);
+        } else {
+            //TODO: This should be decided algorithmically in the future.
+            //ixgbe_tx_map(tx_ring, first, hdr_len);
+            ixgbe_tx_map_hdr_ring(tx_ring, first, hdr_len);
+        }
 
 	return NETDEV_TX_OK;
 
